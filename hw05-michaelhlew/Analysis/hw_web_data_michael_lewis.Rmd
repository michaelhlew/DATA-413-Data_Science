---
title: 'DATA-413/613 Homework on Web Data: APIs and Scraping'
author: "Michael Lewis"
date: "`r Sys.Date()`"
output:
  html_document:
    number_sections: yes
---

# Instructions {.unnumbered}

-   This is a two-week assignment.

-   Write your solutions **in this starter file** and change the file the name and the "author" field in the YAML header.

-   Commit R Markdown and HTML files (no PDF files). **Push both .Rmd and HTML files to GitHub**.

-   Make sure you have knitted to HTML for your final submission.

-   **Only include necessary code and data** to answer the questions.

-   Most of the functions you use should be from the tidyverse. **Too much base R** will result in point deductions.

-   Submit a response on Canvas that your assignment is complete on GitHub

-   Feel free to use Pull requests and or email (attach your .Rmd) to ask me any questions.

-   **You can choose to answer either question 2.6 or 2.7 - no need to do both.**

# Using APIs

1.  Pick a website of your choice (not discussed in class) that requires a free API key and has recent data. You may use an R package for the website or use {httr} to interact directly with the API.

API: <https://covidactnow.org/> Tracks data on COVID cases, deaths, vaccines for the United States.

2.  Identify the link to instructions to obtain a key for the API.

Access: <https://covidactnow.org/data-api> Click "register" Complete the two questions: email / API usage. After submitting the form you will be provided with an API key on the webpage and via email.

3.  Use your API with {keyring} to download a data set with multiple variables. Do not include your key in your file.

```{r message=FALSE}
#Setup
library(tidyverse)
library(httr)
library(jsonlite)
library(keyring)
library(lubridate)
library(data.table)
```

API documentation: <https://apidocs.covidactnow.org> The API documentation provided numerous endpoint URLs for different data formats. I chose to query an endpoint that gathered data for all counties in a state.

Endpoint from website: <https://api.covidactnow.org/v2/county/%7Bstate%7D.json?apiKey=YOUR_KEY_HERE>

I chose to look at my home state Maryland (MD).

```{r}
url1 <- 'https://api.covidactnow.org/v2/county/MD.timeseries.json?apiKey='
API_URL <- paste0(url1, key_get("COVID")) 

api_output <- GET(API_URL) #calling the API

class(api_output$content) 
```

4.  Convert elements of interest into a tibble

```{r}
MD_COVID_data <- fromJSON(rawToChar(api_output$content), flatten = TRUE)
as_tibble(MD_COVID_data) 
glimpse(MD_COVID_data) 
#I am interested in timeseries data which is in a nested df. 
#Each county has a dataframe of timeseries data. 
MD_COVID_data %>% 
  filter(fips == 24027) %>% #the code for my home county
  select(actualsTimeseries) -> howardcounty.df #the data frame w/ time series data
  howardcounty.df$actualsTimeseries[[1]] -> df #making own df

#Basic Data cleaning
  df %>% 
    mutate(date = ymd(date)) -> df
```

5.  State a question of interest.

When did COVID-19 cases and vaccinations peak in my hometown?

6.  Create an appropriate plot with proper labels and theme to analyze your question of interest.

```{r}

df %>% 
  select(date,newCases, newDeaths) %>%
  ggplot(aes(x= date, y = newCases)) + 
  geom_point(color = rgb(0.4,0.4,0.8,0.6), alpha = .75) + 
  theme_bw() + 
  ggtitle("COVID Cases for Howard County, MD", subtitle = "Cases peaked during early 2022 Omicron wave") + 
  labs(x = "Date", y = "Daily New Cases", caption = "Data provided by: CovidActNow") -> p1


df %>% 
  select(date,newCases, vaccinationsCompleted, vaccinationsAdditionalDose) %>%
  ggplot(aes(x= date, y = vaccinationsCompleted)) + 
  theme_bw() + 
  geom_line(aes(color = "Completed Vaccinations")) +
  geom_line(aes(y = vaccinationsAdditionalDose, color = "Additional Dose (Booster)")) +
  scale_y_continuous() + 
  labs(x = "Date", y = "Daily Count", color = "", caption = "Data provided by: CovidActNow") +
  ggtitle("COVID Vaccinations & Booster for Howard County, MD", subtitle = "Boosters lag behind vaccinations") -> p2

p1
p2
```

7.  Interpret the plot to answer your question.

The Omicron wave was by far the peak for COVID cases in my home county. Cases exponentially increased in January 2022 before dramatically falling. As for vaccinations, after July of 2021 boosters started to plateau. Booster shots rapidly increased during the heigh of the Omicron wave but also plateaued shortly thereafter.

8.  Read the site guidance for use of their data, e.g., terms and conditions, API documentation, or FAQ (if they exist)) and, in a few sentences, comment on the severity or strictness of the guidance or the lack of guidance. Discuss how the guidance could enable or restrain unethical uses of the data.

CovidActNow's API isn't very strict. The data are licensed under the Creative Commons (Attribution-NonCommercial-NoDerivatives 4.0 International) which means individuals may share, copy, and reproduce the material in any medium and format so long as credit is given and the data is not used for any commercial purposes. Per their website, "Our non-commercial users can freely download, use, share, modify, or build upon our source code." 

The guidelines restrain unethical use by making it clear that a commercial entity should not use an individual API license for commercial purposes. However, their guidelines don't provide any restrictions beyond this. I can't think of any malicious uses of this COVID data but if an individual wanted they could potentially use this data for purposes not intended by the creators due to the lax terms. 

# IMDB List of Best Foreign Films

IMDB has a list of the [Top 100 Best Foreign Films](https://www.imdb.com/list/ls062615147/). This question requires scraping the following elements from the webpage and creating a tibble with all of these elements.

-   Number
-   Title
-   Year
-   MPAA Rating
-   Length in minutes
-   Genre
-   Star Rating
-   Metascore Rating
-   Gross Receipts
-   Votes
-   Country

**Extra Credit Option**:

-   Add the Directors to the above list of elements.
-   Note: **This is not a trivial change.** Do not attempt until until you have completed the main questions without directors.
-   You can reuse most of your code unchanged. You will need to make the following adjustments.
    -   Adjust the creation of the nodes from elements to add the css for directors - straightforward
    -   Write new code for the logicals for directors and then title - **tricky.**
        -   Multiple movies have more than one director. You will have to identify them and create two variables, one for the first director and one for the second. If not, you will have ambiguous results when reshaping and turn your tibble content into list columns.
    -   Adjust the code for tidying to include directors - straightforward.

**Required Questions**

1.  Scrape the following elements, convert the scraped data into a tibble, and add logical variables to identify which rows belong to which variable. You can use various approaches to accomplish this. The following steps are suggested to guide your work.

<!-- -->

a.  Download the entire web page and scrape the required elements and save the resulting html object into a variable.

```{r}
library(rvest)
read_html("https://www.imdb.com/list/ls062615147/") -> imdb
```

b.  Create a reference set of individually scraped elements  

-   Create a vector of the variable names of interest called `my_vars`
-   Create a vector of the CSS tags for the variable of interest called `my_css`.
    -   These are the tags I used so you can check what you get for each variable.
        -   .genre
        -   .lister-item-header a
        -   .text-primary
        -   .ipl-rating-star.small .ipl-rating-star\_\_rating
        -   .text-muted.unbold
        -   .ghost\~ .text-muted+ span
        -   .mode-detail .list-description p
        -   .text-muted+ span:nth-child(2)
        -   .list-description p
        -   .metascore
        -   .runtime

```{r}
my_vars <- c("genre", "title", "number", "star.rating", "year", "gross.reciepts", "country", "votes", "mpaa.rating", "metascore.rating", "runtime")
  
c(".genre", ".lister-item-header a", ".text-primary",".ipl-rating-star.small .ipl-rating-star__rating", ".text-muted.unbold", ".ghost~ .text-muted+ span", ".mode-detail .list-description p", ".text-muted+ span:nth-child(2)", ".certificate", ".metascore", ".runtime") -> my_css

my_css %>% 
map(~html_nodes(imdb, css =.)) -> movie_list
names(movie_list) <- my_vars

#Length
movie_list %>% 
  map_dbl(length) 

#Sum
movie_list %>% 
  map_dbl(length) %>% 
  sum()
```

-   Use `map()` to scrape each element in `my_css` into a list called `movie_list`.

-   Assign names for the list elements in `movie_list` using the values from `my_vars`.

-   Use `map_dbl()` to create a named vector of the length of each item in the list and sum the total.

-   Your output should look *similar* to the following.

-   Note that some elements have more than 100 elements and some have less. The ones with more are due to the CSS identifying elements that may not have been visible in Selector Gadget. You will remove them later.

-   The ones with fewer than 100 elements are where some movies are missing a value but we don't know for which movies we have values.

c.  Use a {stringr} function to collapse `my_css` into a single value called `css_values` containing all of the css elements separated by ","s.

```{r}
my_css %>% 
  str_flatten(collapse = ",") -> css_values
```

d.  Use `css_values` to scrape all of the elements at once into a single html object.

```{r}
html_nodes(imdb, css = css_values) -> movies_object
```

e.  Create a new variable with the text from each element.  

-   Confirm that the length of the vector is the same as the sum of the lengths of the elements from `movie_list`
-   Use head() to check the first 12 values.
-   Remove any values prior to the number of the first movie (1) which should be followed by "City of God"

```{r}
html_text(movies_object) -> movies_text
movies_text %>% 
  length() #Length matches
```

```{r}
movies_text[9:1008] -> movies_text
```

f.  Create a tibble from the text and use a {stringr} function to remove any extra white space in the text or on either end.

```{r}
tibble(text = movies_text) -> movie.df

movie.df %>% 
  mutate(text = str_squish(text)) -> movie.df
```

g.  Create logical variables to uniquely identify the rows for each variable. Discard any non-movie-related data prior to the first row with movie data.

```{r}

movie.df %>% 
  mutate( 
    rank = str_detect(text, "^\\d+\\.$"),
    year = str_detect(text, "\\(\\d+\\)"),
    runtime = str_detect(text, "^\\d+\\smin$"),
    country = str_detect(text, "^From\\s."),
    gross.reciepts = str_detect(text, "^\\$"),
    star.rating = str_detect(text, "^\\d\\.\\d$|^\\d$"),
    metascore.rating = str_detect(text, "^[3-9][0-9]$"),
    mpaa.rating = str_detect(text, "^\\D$|^\\D\\D$|^\\w\\w\\-\\w\\w$|^Not\\s+|Passed|Unrated"),
    votes = !metascore.rating & !star.rating & str_detect(text, "^\\d+$|^\\d+\\,\\d+$"), 
    genre = str_detect(text,"^[:alpha:]+\\,\\s[^...]+$|Drama|Comedy"),
    title = !rank& !year & !runtime & !country & !gross.reciepts & !star.rating & !metascore.rating & !mpaa.rating & !votes & !genre) -> movie.df

```

2.  In one continuous set of code summarize the number of data entries missing for the elements for each movie by using `across()` with an anonymous function and then reshape and arrange in descending order. You should have only three elements missing entries from the movies with a total of 100 that are missing.

-   In a second set of code, show your total of missing entries is as you expected given 100 movies with 11 elements each (not counting directors).

```{r}
movie.df %>% 
select(-text) -> movie.df2
movie.df2 %>%
mutate(across(.cols = everything(), .fns = ~ sum(., na.rm = TRUE))) %>% 
distinct() -> missingvals 

#Cleaning it up 
transpose(missingvals) -> missingvals
rownames(missingvals) <- colnames(movie.df2)
missingvals %>% 
  mutate(missing = 100-V1) %>% 
  select(-V1) %>% 
  arrange(desc(missing))

#Checking work 
sum(41+44+15)

```

3.  Use {dplyr} and {tidyr} functions to tidy/reshape the tibble, without the logical variables, and save into a new tibble.

```{r}
movie.df %>%
  mutate(key = case_when(
    rank ~ "rank",
    title ~ "title",
    year ~ "year",
    genre ~ "genre",
    metascore.rating ~ "metascore.rating", 
    runtime ~ "runtime",
    star.rating ~ "star.rating",
    country ~ "country",
    votes ~ "votes",
    mpaa.rating ~ "mpaa.rating",
    gross.reciepts ~ "gross.reciept"
  )) %>%
  
  mutate(movienum = cumsum(rank)) %>%
  select(key, text, movienum) %>%
  pivot_wider(names_from = key, values_from = text) -> movie.df

movie.df

```

4.  Use {readr} parse functions and {stringr} functions to clean the data and save back to the tibble.  

-   Eliminate any extra variables and characters.
-   Ensure all apparent numbers are numeric.
-   Use a {readr} function to convert `country` into a factor.
-   Show the first 6 rows of the tibble and visually check the class for each variable.

```{r}
movie.df %>% 
  select(-movienum) %>% #Movienum no longer needed
  mutate(year = parse_number(year)) %>% 
  mutate(star.rating = parse_number(star.rating)) %>%
  mutate(metascore.rating = parse_number(metascore.rating)) %>% 
  mutate(votes = parse_number(votes)) %>%
  mutate(gross.reciept.millions = parse_number(gross.reciept)) %>% 
  select(-gross.reciept) %>% 
  mutate(runtime.min = parse_number(runtime)) %>% 
  select(-runtime) %>% 
  mutate(genre = str_squish(genre)) %>% 
  #cleaning up country 
  mutate(country = str_replace_all(country, "^From", "")) %>%
  mutate(rank = str_replace_all(rank, ".$", "")) %>% 
  mutate(rank = parse_number(rank)) %>%
  mutate(country = parse_factor(country)) -> movie.df

head(movie.df) #first six rows

glimpse(movie.df) #checking class

```

5.   

-   

    a.  Create a plot of the length of a film and its gross, color coded by rating, where you *filter out any MPAA category with less than 4 films*. Add a linear smoother *for each rating*.

```{r}
#Checking which MPAA
movie.df %>% 
  select(mpaa.rating) %>%
  mutate(mpaa.rating = parse_factor(mpaa.rating)) %>%
  group_by(mpaa.rating) %>%
  mutate(n = n()) %>% 
  distinct() %>% 
  filter(n < 4) 

filter_outs <- c("G", "TV-MA", "TV-PG", "GP", "TV-14", "Passed", "Unrated")
`%ni%` <- Negate(`%in%`)

movie.df %>% 
  filter(mpaa.rating %ni% filter_outs) %>%
  ggplot(mapping = aes(x = runtime.min, y = gross.reciept.millions, color = mpaa.rating)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  theme_bw() +
  labs(x = "Film Length in Minutes",
       y = "Gross Receipts in Millions USD") 
```

-   

    b.  Interpret the plot to answer the question: **for which MPAA ratings (if any) is there a positive or negative relationship between length of a film and its gross revenue?**

Based on the plot it appears that there is a positive relationship between length (in minutes) and gross receipts (Millions USD) for (in order of decreasing relationship strength): PG-13, PG, and Not Rated. There does not appear to be any linear relationship for R and N/A.

**Reminder - Choose Question 6 or 7**

6.  

-   

    a.  Create a plot of `stars` versus `metacritic score`, color coded by MPAA rating (i.e., for predicting stars rating based on meta-critic scores).\

-   

```{r}
movie.df %>% 
  ggplot(mapping = aes(x = metascore.rating, y = star.rating, color = mpaa.rating)) +
  geom_point() +
  theme_bw() 
```

    b.  Include a single Ordinary Least Squares smoothing line with no standard errors and interpret the plot.\
    
```{r}
movie.df %>% 
  ggplot(mapping = aes(x = metascore.rating, y = star.rating, color = mpaa.rating, group=1)) +
  geom_point() +
  theme_bw() + 
  geom_smooth(method = lm, se = FALSE)
```

Interpretation: there appears to be a moderate positive association between metacritic rating and star ratings. 

-   

    c.  Use a linear model to assess if there is there a meaningful relationship.

    -   Show the summary of the output and interpret in terms of the $p$-value and the adjusted R-Squared.
    -   Explain why you are surprised at the result (or not) based on the plot.
    
```{r}
lm(star.rating~metascore.rating, data = movie.df) -> lm.movie
summary(lm.movie) 
```
In a linear test aimed at predicting star.ratings from metacritic ratings, we get a p-value of .375 which is greater than any commonly used alpha. Thus, we reject the null-hypothesis and conclude there is not a statistically significant linear relationship. Additionally, we got an adjusted r-sq value of -.003. This indicates that the predictors explanatory power towards the responnse is extremely low if there is any at all. 

I am surprised at the result because the plot indicated that there might be some linear relationship. The plot didn't show a strong linear association, however, a moderate linear relationship looked likely. Further, the beta for metascore.rating is .005 which is way smaller than I would have expected based on the graph. 

7.  

-   

    a.  Create a box plot of gross receipts by MPAA rating where MPAA is sorted by the maximum gross to answer which MPAA rating has the highest median gross receipts?

-   

    b.  Which R-rated movies are in the *overall top 10* of gross receipts?

-   

    c.  How many did you expect based on the box plot and why is the plot deceptive?

-   

    d.  Use one-way analysis of variance to assess the level of evidence for whether all ratings have the same mean gross receipts. Show the summary of the results and provide your interpretation of the results.

8.  Reproduce the following plot (in the HTML) as closely as possible and interpret the plot.

```{r}
movie.df %>% 
  mutate(country = fct_reorder(as.factor(country),votes, .fun = max)) %>%
  group_by(country) %>%
  ggplot(aes(x = votes, y = country)) + 
  geom_boxplot() + 
  scale_x_log10() + 
  ggtitle("IMDB Votes per Top 100 Foreign Films by Country") + 
  labs(x = "Votes", y = "") + 
  theme_bw()
```

Interpretation: Generally, the top 7 countries had a median number of votes around 1e+05 (logged scale). Films from Japan appear to have the most variability when it comes to votes. It also seems like a set of 9 countries produce many more movies that are in the Top 100 than the rest. Indeed, it seems like most countries have just one point for votes indicating that these countries may produce comparatively less films in the Top 100. 


**Extra Credit**: Only for those that scraped the Directors

-   Identify the top 5 first directors based on their total number of movies and show their total gross
-   Identify the top 5 first directors based on their total gross and the number of movies
-   By just looking at the results, which director appears on both lists?

# Extra Credit 1 Pts

-   Listen to the AI Today podcast on [Machine Learning Ops](https://podcasts.apple.com/us/podcast/ai-today-podcast-artificial-intelligence-insights-experts/id1279927057?i=1000468771571) and provide your thoughts on the following questions:

1.  Does knowing about Git and GitHub help you in understanding the podcast?

Yes, one of the things Marsen mentioned was the idea that data science was having a reproducability crisis as well as the fact that Git/GitHub allowed for rapid iteration and development. Both points highlight the importance of these tools. GitHub for example allows anyone to recreate a given project and edit it if needed. Thus, these concepts did help me understand some of the concepts Marsden covers. 

2.  How do you think the ideas of ML OPs will affect your future data science projects?

Marsden discusses how there should be one ML OPs team that is multidisciplinary with the goal of bringing ML models to production. I think this premise will mean that my future data science projects (especially in the workforce) will depend on more multifaceted/taleneted teams to quickly adapt and respond to challenges in the ML context.

You may also want to check out this article on [Towards Data Science](https://towardsdatascience.com/ml-ops-machine-learning-as-an-engineering-discipline-b86ca4874a3f)
